aka Trasfer Function.

### Why ?
Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. Without the differentiable non linear function, this would not be possible.



A neural network without an activation function is essentially just a linear regression model. When we do not have the activation function the weights and bias would simply do a linear transformation. A linear equation is simple to solve but is limited in its capacity to solve complex problems. 


### Activation Function
Activation Functionn basically decide whether a neuron should be activated or not. Whether the information that the neuron is receiving is relevant for the given information or should it be ignored.

https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6
https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
https://arxiv.org/pdf/1710.05941.pdf searching an activation function
