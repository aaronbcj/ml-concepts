{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained GloVe Embedding\n",
    "\n",
    "GloVe: Global Vectors for Word Representation.\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. \n",
    "\n",
    "This dataset contains English word vectors pre-trained on the combined Wikipedia 2014 + Gigaword 5th Edition corpora (6B tokens, 400K vocab). \n",
    "All tokens are in lowercase. This dataset contains 50-dimensional, 100-dimensional and 200-dimensional pre trained word vectors.\n",
    "\n",
    "Ref:\n",
    "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. \n",
    "https://nlp.stanford.edu/pubs/glove.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files can be downloaded from https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation/data\n",
    "\n",
    "For this notebook i have used **glove.6B.50d.txt**, which contains a 50-dimensional version of the embedding.\n",
    "\n",
    "#### If you open above file you will see a token (word) followed by the weights (50 numbers) on each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def load_glove(fileName):\n",
    "    embeddings_map = {}\n",
    "    word_map={}\n",
    "    i=0\n",
    "    \n",
    "    print('loading word embedding file')\n",
    "    \n",
    "    with open(os.path.join(\".\", fileName)) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_map[word] = word_embedding\n",
    "            word_map[word]=i\n",
    "            i = i + 1\n",
    "    \n",
    "    print('Found %s word vectors.' % len(embeddings_map))\n",
    "    return embeddings_map, word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embedding file\n",
      "Found 400000 word vectors.\n",
      "Print details for word: where\n",
      "[  6.92369998e-01   4.49710011e-01  -2.02930003e-01  -1.67830005e-01\n",
      "   3.05029988e-01  -4.87599999e-01  -6.90280020e-01   1.81630000e-01\n",
      "  -1.62949994e-01  -4.74770010e-01  -3.30440002e-03  -6.52079999e-01\n",
      "  -1.01480000e-01  -5.75100005e-01   3.01889986e-01   3.56389999e-01\n",
      "   2.86289990e-01   4.73670006e-01  -7.14559972e-01  -1.88650005e-02\n",
      "   1.70959994e-01   2.70969987e-01   1.90709993e-01   7.63260007e-01\n",
      "  -1.75860003e-01  -1.79809999e+00  -3.37220013e-01   2.73250014e-01\n",
      "   5.40950000e-02  -5.23500025e-01   3.49079990e+00  -2.76190005e-02\n",
      "  -2.39490002e-01  -8.69759977e-01   2.66119987e-01   8.79559964e-02\n",
      "  -1.98850006e-01   1.85340002e-01   4.32500005e-01   4.12079990e-01\n",
      "  -3.91900003e-01   2.28569999e-01   7.34430030e-02   1.09010004e-01\n",
      "  -2.34950006e-01   1.60820007e-01  -1.63640007e-02  -1.03470004e+00\n",
      "  -2.41600007e-01  -4.86799985e-01]\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "embeddings_map, word_map = load_glove('glove.6B.50d.txt')\n",
    "print('Print details for word: {}'.format('where'))\n",
    "print(embeddings_map['where'])\n",
    "print(word_map['where'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we have our word vector.\n",
    "\n",
    "As an example, let's take a sample input sentence and construct its vector representation aka **Embedding Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10 # just for a sample sentence\n",
    "MAX_DIMENSION = 50 # we have used 50 dimension embedding of GloVe\n",
    "\n",
    "def generate_embedding_matrix(sentence):\n",
    "    words = sentence.split()  # string array\n",
    "    embedding_matrix = np.zeros((MAX_NUM_WORDS), dtype='int32')   # method response\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "            \n",
    "        embedding_vector = word_map[words[i]] \n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence vector obtained after replaceing words with integer representation from word vector\n",
      "[ 373   41  913 2518  353    0    0    0    0    0]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "sentence = 'today i am feeling great'\n",
    "print('sentence vector obtained after replaceing words with integer representation from word vector')\n",
    "matrix = generate_embedding_matrix(sentence)\n",
    "print(matrix)\n",
    "print(matrix.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
